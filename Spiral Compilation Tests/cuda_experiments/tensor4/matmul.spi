open corebase
open corecuda
open tensorm

nominal gemm_config m_frag n_frag k_frag frag_in a_layout b_layout float = 
    {m_frag : int; n_frag : int; k_frag : int; m_tile : int; n_tile : int; k_tile : int; a_trans : bool; b_trans : bool}
inl gemm_config forall m_frag n_frag k_frag frag_in a_layout b_layout float. {m_tile n_tile k_tile} 
        : gemm_config m_frag n_frag k_frag frag_in a_layout b_layout float = 
    gemm_config {
        m_tile n_tile k_tile
        m_frag = real real_core.type_lit_to_lit `m_frag
        n_frag = real real_core.type_lit_to_lit `n_frag
        k_frag = real real_core.type_lit_to_lit `k_frag
        a_trans = (real typecase a_layout with wmma.row_major => true | wmma.col_major => false) : bool
        b_trans = (real typecase b_layout with wmma.row_major => true | wmma.col_major => false) : bool
        }

type warp_size = 32
inl warp_size() : int = real real_core.type_lit_to_lit `warp_size

inl wmma_gemm_shared forall m_frag n_frag k_frag frag_in a_layout b_layout float {number}. 
        (gemm_config {m_frag n_frag k_frag m_tile n_tile k_tile a_trans b_trans} : gemm_config m_frag n_frag k_frag frag_in a_layout b_layout float) 
        (alpha : float) (a : tensor (int * int) float) (b : tensor (int * int) float) 
        (beta : float) (c : tensor (int * int) float) =
    inl swap (a,b) x = if x then b,a else a,b
    inl _ =
        inl a_dim = swap a.dim a_trans
        inl b_dim = swap b.dim b_trans
        inl c_dim = c.dim
        assert (snd a_dim = fst b_dim) "The K dimension of the A and B tensors must match."
        assert (fst a_dim = fst c_dim) "The M dimension of the A and C tensors must match."
        assert (snd b_dim = snd c_dim) "The N dimension of the B and C tensors must match."

    inl a = 
        if a_trans then // col_major
            reshape (fun k,m => {k=k / k_tile}, {k_tile}, {m=m / m_tile}, {m_tile}) a
            |> reorder (fun k,{k_tile},m,{m_tile} => m,k,(k_tile,m_tile))
        else // row_major
            reshape (fun m,k => {m=m / m_tile}, {m_tile}, {k=k / k_tile}, {k_tile}) a
            |> reorder (fun m,{m_tile},k,{k_tile} => m,k,(m_tile,k_tile))
    inl b =
        if b_trans then // col_major
            reshape (fun n,k => {n=n / n_tile}, {n_tile}, {k=k / k_tile}, {k_tile}) b
            |> reorder (fun n,{n_tile},k,{k_tile} => n,k,(n_tile,k_tile))
        else // row_major
            reshape (fun k,n => {k=k / k_tile}, {k_tile}, {n=n / n_tile}, {n_tile}) b
            |> reorder (fun k,{k_tile},n,{n_tile} => n,k,(k_tile,n_tile))
    inl c = // row_major
        reshape (fun m,n => {m=m / m_tile}, {m_tile}, {n=n / n_tile}, {n_tile}) c
        |> reorder (fun m,{m_tile},n,{n_tile} => (m,n),(m_tile,n_tile))
    
    inl bank_size = warp_size() * 4 / (sizeof : _ float).value 
    inl threads_per_block = 1024
    inl blocks_per_grid = 8
    inl shared_mem = 
        inl fragments_a = (m_tile * k_tile) / (m_frag * k_frag) 
        inl fragments_b = (k_tile * n_tile) / (k_frag * n_frag) 
        inl fragments_c = (m_tile * n_tile) / (m_frag * n_frag)
        inl padding_a = (swap (m_frag, k_frag) a_trans |> snd) % bank_size
        inl padding_b = (swap (k_frag, n_frag) b_trans |> snd) % bank_size
        inl padding_c = n_frag % bank_size
        inl shared_a = m_tile * k_tile + (fragments_a - 1) * padding_a
        inl shared_b = k_tile * n_tile + (fragments_b - 1) * padding_b
        inl shared_c = m_tile * n_tile + (fragments_c - 1) * padding_c
        (shared_a + shared_b + shared_c) * (sizeof : _ float).value
    // print_static shared_mem
    run' {blocks_per_grid threads_per_block shared_mem} fun () =>
        global "#include <mma.h>"
        global "using namespace nvcuda;"
        global "#include <cooperative_groups.h>"
        global "#include <cooperative_groups/memcpy_async.h>"
        global "using namespace cooperative_groups;"

        open cooperative_groups
        open tensorm.cuda

        inl block = create_block()
        inl warp : _ (_ warp_size _) = create_thread_block_tile block
        inl create_shared dim : tensor _ float = tensor_create_shared dim
        
        loop_blocks_in_grid' (fst a.dim, fst b.dim) fun m,n =>
            inl a = apply m a
            inl b = apply n b
            inl c = apply (m,n) c
            
            open wmma
            inl c_shared' = create_shared (m_tile, n_tile)
            inl c_frag : fragment accumulator m_frag n_frag k_frag row_major float = create_fragment
            async_memcpy_tensor warp {from=c; to=c_shared'}
            inl c_shared = // row_major
                reshape_fst (fun m => {m=m / m_frag}, {m_frag}) c_shared'
                |> reshape_snd (fun n => {n=n / n_frag}, {n_frag})
                |> reorder (fun (m,{m_frag}),n,{n_frag} => (m,n),(m_frag,n_frag))

            loop_linear' (fst a.dim) fun k =>
                // The multiplication by the beta should only be done once. On the rest of the K iterations 
                // it should be adding the accumulator results directly.
                inl beta = if k = {k=0} then beta else 1

                inl a = apply k a
                inl a_shared' = create_shared (swap (m_tile, k_tile) a_trans)
                inl a_frag : fragment matrix_a m_frag n_frag k_frag a_layout frag_in = create_fragment
                sync block
                async_memcpy_tensor warp {from=a; to=a_shared'} 
                inl a_shared = 
                    if a_trans then // col_major
                        reshape_fst (fun k => {k=k / k_frag}, {k_frag}) a_shared'
                        |> reshape_snd (fun m => {m=m / m_frag}, {m_frag})
                        |> reorder (fun (k,{k_frag}),m,{m_frag} => m,k,(k_frag,m_frag))
                    else // row_major
                        reshape_fst (fun m => {m=m / m_frag}, {m_frag}) a_shared'
                        |> reshape_snd (fun k => {k=k / k_frag}, {k_frag})
                        |> reorder (fun (m,{m_frag}),k,{k_frag} => m,k,(m_frag,k_frag))

                inl b = apply k b
                inl b_shared' = create_shared (swap (k_tile, n_tile) b_trans)
                inl b_frag : fragment matrix_b m_frag n_frag k_frag b_layout frag_in = create_fragment
                async_memcpy_tensor warp {from=b; to=b_shared'} 
                inl b_shared =
                    if b_trans then // col_major
                        reshape_fst (fun n => {n=n / n_frag}, {n_frag}) b_shared'
                        |> reshape_snd (fun k => {k=k / k_frag}, {k_frag})
                        |> reorder (fun (n,{n_frag}),k,{k_frag} => n,k,(n_frag,k_frag))
                    else // row_major
                        reshape_fst (fun k => {k=k / k_frag}, {k_frag}) b_shared'
                        |> reshape_snd (fun n => {n=n / n_frag}, {n_frag})
                        |> reorder (fun (k,{k_frag}),n,{n_frag} => n,k,(k_frag,n_frag))
                wait warp . sync block

                loop_warps_in_block' (fst a_shared.dim, fst b_shared.dim) fun m,n =>
                    inl a = apply m a_shared
                    inl b = apply n b_shared
                    inl c = apply (m,n) c_shared

                    inl acc : fragment accumulator m_frag n_frag k_frag row_major float = create_fragment
                    fill_fragment acc 0
                    loop_linear' (fst a.dim) fun k =>
                        inl a = apply k a
                        inl b = apply k b

                        load_matrix_sync a_frag a
                        load_matrix_sync b_frag b

                        mma_sync acc a_frag b_frag acc

                    load_matrix_sync c_frag c
                    loop.linear' (length c_frag) fun i => // TODO: Optimize this.
                        set c_frag i (alpha * index acc i + beta * index c_frag i)
                        
                    store_matrix_sync c c_frag
                    
            sync block
            
            async_memcpy_tensor warp {from=c_shared'; to=c}

type a_layout = wmma.row_major
type b_layout = wmma.row_major

inl main() =
    inl get_body forall dim el. (x : tensor dim el) : array el = 
        real tensorm.utils.map (fun (tensor_body {array}) => array) x.bodies : array el

    inl random_normal dim =
        inl len : int = real tensorm.utils.prod dim
        inl t : array f32 = $"cp.random.normal(0,1,!len,cp.float32)" 
        fromArray t |> reshape (const dim)

    inl swap (a,b) x = if x then b,a else a,b
    inl cp_matmul (a : tensor (int * int) float * bool) (b : tensor (int * int) float * bool) (c : tensor (int * int) float) : tensor (int * int) float =
        inl transpose a_trans (x : array float) : array float = if a_trans then $"cp.transpose(!x)" else x
        inl f (a_trans : bool) (a_body : array float) (a_dim : int * int) : array float = 
            inl x : array float = $"!a_body.reshape(!a_dim)"
            transpose a_trans x
        inl g (a,a_trans : tensor (int * int) float * bool) = f a_trans (get_body a) a.dim
        inl a_body,b_body,c_body : array float * array float * array float = g a, g b, f false (get_body c) c.dim
        
        $"(cp.matmul(!a_body,!b_body)).flatten()"
        |> fromArray |> reshape (const c.dim)

    inl m,n,k : int * int * int = 256, 256, 256
    inl a_trans = (real typecase a_layout with wmma.row_major => true | wmma.col_major => false) : bool
    inl b_trans = (real typecase b_layout with wmma.row_major => true | wmma.col_major => false) : bool

    inl [a; b; c] = listm.map random_normal ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl d = cp_matmul (a,a_trans) (b,b_trans) c
    // console.write_ln (zip a b)
    inl gemm_config_tf32 = gemm_config {m_tile=64; n_tile=128; k_tile=64} : gemm_config 16 16 8 wmma.tf32 a_layout b_layout f32
    wmma_gemm_shared gemm_config_tf32 1 a b 0 c
    inl d,c = get_body d, get_body c
    global "from max_blocks_per_sm import max_blocks_per_sm"
    $"max_blocks_per_sm(cp.cuda.Device(),raw_module.get_function('entry0'),1024,is_print=True)" : ()

    $"cp.max(cp.abs(!c-!d))" : f32