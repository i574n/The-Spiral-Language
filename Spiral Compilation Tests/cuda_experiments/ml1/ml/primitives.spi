open corebase
open corecuda
open rangem
open tensorm

// The primitives here are only attended for execution on the GPU.
// They're all block level primitives. 

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the map kernel need to be the same."
    inl from,to = factorize_sizeof_16 from, factorize_sizeof_16 to
    loop.projective threads_in_block(fst from.dim) fun i => 
        inl from, to = apply i from, apply i to
        inl l_from, l_to = tensor_create from.dim, tensor_create to.dim
        memcpy_sync (l_from, from)
        pragma.unroll fun _ =>
            loop.linear from.dim fun j => 
                tensor_set j (tensor_index j l_from |> f) l_to
        memcpy_sync (to, l_to)

    __syncthreads()

inl block_reduce_store forall a. f result (to : tensor int a) = 
    assert (1 = to.dim) "The output answer has to be of size 1."
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups

    // The final result for each of the warps.
    inl warp_result = reduce_coalesced create_coalesced_threads() f result
 
    open tensor_cuda
    inl warps_in_block = warps_in_block()
    inl shared = tensor_create_shared warps_in_block.by

    // All the warps in a block store their intermediate results into the shared tensor.
    tensor_set warps_in_block.from warp_result shared

    __syncthreads()
    
    inl threads_in_warp = threads_in_warp()
    assert (shared.dim <= threads_in_warp.by) "The amount of results in their shared array to be reduced should be less than the number of threads in the warp."
    if warps_in_block.from = 0 && threads_in_warp.from < shared.dim then
        // The first warp reduces the intermediate results in the shared tensor.
        inl final_result = reduce_coalesced create_coalesced_threads() f (tensor_index threads_in_warp.from shared)
        // Stores the final result into global memory.
        tensor_set 0 final_result to
    
// Reduces all the elements of a tensor to a single one, given the neutral element as well as the reducer function.
inl reduce forall dim a. neutral_element (f : a -> a -> a) (from : tensor dim a) (to : tensor int a) : () =
    // The individual threads iterate over the global tensor, reducing the elements as they go along.
    inl from = factorize_sizeof_16 from
    inl result = loop._dup neutral_element
    loop.projective threads_in_block(fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        memcpy_sync (local, from)
        loop.for {from=0; nearTo=local.dim} (fun i => f (tensor_index i local)) result
        |> loop._set result

    block_reduce_store f result to
    __syncthreads()

type template_2d_config = {
    threads_per_miniblock : int
    miniblock : ref cooperative_groups.thread_block_tile'
    }
inl block_reduce_2d forall b. 
        ({threads_per_miniblock miniblock} : template_2d_config)
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups
    // The final result for each of the warps.
    if threads_per_miniblock <= threads_per_warp() then
        reduce_thread_block_tile' miniblock f result
    else
        open tensor_cuda
        // Todo: replace `reduce_coalesced` with `reduce_thread_block_tile'`. 
        inl warp_result = reduce_coalesced create_coalesced_threads() f result
        inl warps_in_block = warps_in_block()
        inl shared = tensor_create_shared (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_result shared

        sync miniblock
        
        inl threads_in_warp = threads_in_warp()
        reduce_coalesced create_coalesced_threads() f (
            if threads_in_warp.from < snd shared.dim then
                tensor_index (fst index_warp, threads_in_warp.from) shared
            else
                neutral_element
            )

inl local_map forall a b. f (from : tensor (int * int) a) : tensor (int * int) b = 
    inl to = tensor_create from.dim
    loop.linear from.dim fun i =>
        tensor_set i (f (tensor_index i from)) to
    to

inl local_reduce forall a. config (neutral_element : a) f (from : tensor (int * int) a) : a = 
    // Reduces the individual thread elements. 
    inl result = loop._dup neutral_element
    loop.linear from.dim fun i =>
        f (tensor_index i from) result
        |> loop._set result

    // Reduces the blockwise results. Broadcasts the final result to all the threads.
    block_reduce_2d config neutral_element f result

inl local_sum config = local_reduce config 0 (+)
inl local_prod config = local_reduce config 1 (*)
inl local_max config = local_reduce config limit.min max
inl local_min config = local_reduce config limit.max min

inl template_2d forall dim a b.
        (block : ref cooperative_groups.thread_block)
        (f : template_2d_config -> tensor (int * int) a -> tensor (int * int) b)
        (from : tensor (dim * int) a) (to : tensor (dim * int) b) : () =
    assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal."

    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups

    inl from =
        from 
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
    inl (dim_block,dim_local),sizeof_16 = from.dim
    inl threads_per_miniblock = snd dim_block
    inl miniblock = create_thread_group_from_thread_block block threads_per_miniblock
    inl from =
        from
        |> curry_fst
        |> threads_in_block 
        |> apply_proj 
        |> curry_fst

    inl to =
        to 
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
        |> curry_fst
        |> threads_in_block 
        |> apply_proj 
        |> curry_fst

    loop.linear (fst from.dim) fun i =>
        inl from, to = apply i from, apply i to
        inl local = tensor_create from.dim

        // Loads the tile into the registers.
        loop.linear (fst from.dim) fun i =>
            memcpy_sync (apply i local, apply i from)

        // Map the local tensor using local operations.
        inl local = f {threads_per_miniblock miniblock} local
                
        // Stores the result into global memory after mapping it.
        loop.linear (fst to.dim) fun i =>
            memcpy_sync (apply i to, apply i local)

    __syncthreads()


// Reduces the innermost dimension of a given tensor.
inl reduce_2d forall dim a. neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor dim a) : () =
    assert (fst from.dim = to.dim) "The first dimension of the input has to equal the dimension of the output in the reduce_2d kernel."
    
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from = apply i from |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl from = apply i from
            inl l_from = tensor_create from.dim
            memcpy_sync (l_from, from)
            loop.for {from=0; nearTo=l_from.dim} (fun i => f (tensor_index i l_from)) result
            |> loop._set result

        tensor_set i (reduce_coalesced create_coalesced_threads() f result) to

    __syncthreads()

inl scan_2d_template forall dim a. is_inclusive neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor (dim * int) a) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the inclusive scan kernel need to be the same."
    
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/scan.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from,to = apply i from |> factorize_sizeof_16, apply i to |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl group = create_coalesced_threads()
            inl from,to = apply i from, apply i to
            inl l_from = tensor_create from.dim

            memcpy_sync (l_from, from)
            inl prefix,sum = 
                loop.for {from=0; nearTo=l_from.dim} (fun i => f (tensor_index i l_from)) neutral_element
                |> exclusive_scan_coalesced' group neutral_element f
            
            loop.for {from=0; nearTo=l_from.dim} (fun i s => 
                if is_inclusive then
                    inl s = f (tensor_index i l_from) s
                    tensor_set i s l_from
                    s
                else
                    inl x = tensor_index i l_from
                    tensor_set i s l_from
                    f x s
                ) (f result prefix)
            |> ignore
            memcpy_sync (to, l_from)
            loop._set result (f result sum)

    __syncthreads()

// Scans the innermost dimension of a given tensor.
inl inclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d_template true
// Scans the innermost dimension of a given tensor.
inl exclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d_template false

inl test1() =
    inl m,n : int * int = 3, 36*4
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input : _ _ int = cupy.ones (m,n)
    inl output_inclusive : _ _ int = cupy.zeros (m,n)
    inl output_exclusive : _ _ int = cupy.zeros (m,n)
    inl output_reduce : _ _ int = cupy.zeros m
    run fun _ =>
        map ((+) 1) input input
        exclusive_scan_2d 0 (+) input output_exclusive
        inclusive_scan_2d 0 (+) input output_inclusive
        reduce_2d 0 (+) input output_reduce
    printing.tensor_print_ln 1024 input
    printing.tensor_print_ln 1024 output_exclusive
    printing.tensor_print_ln 1024 output_inclusive
    printing.tensor_print_ln 1024 output_reduce
    ()

inl test2() =
    inl m,n : int * int = 64*2, 16
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input : _ _ float = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input : _ _ int = cupy.ones (m,n)
    inl output_reduce = cupy.zeros 1
    inl output_map_reduce_replicate_map_2d = cupy.zeros (m,n)
    run fun _ =>
        inl block = cooperative_groups.create_block()
        // map ((+) 1) input input
        reduce 0 (+) input output_reduce
        template_2d block (fun config x => 
            inl average = local_sum config x
            inl x = local_map (fun x => average) x
            // inl x = local_map (fun x => exp (x - average)) x
            // inl sum = local_sum config x
            // inl x = local_map (fun x => x / sum) x
            x
            ) input output_map_reduce_replicate_map_2d

    // printing.tensor_print_ln 1024 input
    // printing.tensor_print_ln 1024 output_reduce
    printing.tensor_print_ln 1024 output_map_reduce_replicate_map_2d
    ()

inl main() = test2()