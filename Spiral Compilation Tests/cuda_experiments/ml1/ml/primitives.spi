open corebase
open corecuda
open rangem
open tensorm

// The primitives here are only attended for execution on the GPU.
// They're all block level primitives. 

// Maps all the elements of a tensor given the mapping function.
inl map forall dim a b. (f : a -> b) (from : tensor dim a) (to : tensor dim b) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the map kernel need to be the same."
    inl from,to = factorize_sizeof_16 from, factorize_sizeof_16 to
    loop.projective threads_in_block(fst from.dim) fun i => 
        inl from, to = apply i from, apply i to
        inl l_from, l_to = tensor_create from.dim, tensor_create to.dim
        memcpy_sync (l_from, from)
        pragma.unroll fun _ =>
            loop.linear from.dim fun j => 
                tensor_set j (tensor_index j l_from |> f) l_to
        memcpy_sync (to, l_to)

    __syncthreads()

inl block_reduce_store forall a. f result (to : tensor int a) = 
    assert (1 = to.dim) "The output answer has to be of size 1."
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups

    // The final result for each of the warps.
    inl warp_result = reduce_coalesced create_coalesced_threads() f result
 
    open tensor_cuda
    inl warps_in_block = warps_in_block()
    inl shared = tensor_create_shared warps_in_block.by

    // All the warps in a block store their intermediate results into the shared tensor.
    tensor_set warps_in_block.from warp_result shared

    __syncthreads()
    
    inl threads_in_warp = threads_in_warp()
    assert (shared.dim <= threads_in_warp.by) "The amount of results in their shared array to be reduced should be less than the number of threads in the warp."
    if warps_in_block.from = 0 && threads_in_warp.from < shared.dim then
        // The first warp reduces the intermediate results in the shared tensor.
        inl final_result = reduce_coalesced create_coalesced_threads() f (tensor_index threads_in_warp.from shared)
        // Stores the final result into global memory.
        tensor_set 0 final_result to
    
    __syncthreads()

// Reduces all the elements of a tensor to a single one, given the neutral element as well as the reducer function.
inl reduce forall dim a. neutral_element (f : a -> a -> a) (from : tensor dim a) (to : tensor int a) : () =
    // The individual threads iterate over the global tensor, reducing the elements as they go along.
    inl from = factorize_sizeof_16 from
    inl result = loop._dup neutral_element
    loop.projective threads_in_block(fst from.dim) fun i =>
        inl from = apply i from
        inl local = tensor_create from.dim
        memcpy_sync (local, from)
        loop.for {from=0; nearTo=local.dim} (fun i => f (tensor_index i local)) result
        |> loop._set result

    block_reduce_store f result to

inl block_reduce_2d forall b. 
        (threads_per_miniblock, miniblock : int * ref cooperative_groups.thread_block_tile') 
        (neutral_element : b) 
        (f : b -> b -> b) 
        (result : b) =
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups
    // The final result for each of the warps.
    if threads_per_miniblock <= threads_per_warp() then
        reduce_thread_block_tile' miniblock f result
    else
        open tensor_cuda
        // Todo: replace `reduce_coalesced` with `reduce_thread_block_tile'`. 
        inl warp_result = reduce_coalesced create_coalesced_threads() f result
        inl warps_in_block = warps_in_block()
        inl shared = tensor_create_shared (threads_per_block() / threads_per_miniblock, threads_per_miniblock / threads_per_warp())
        assert (loop.prod shared.dim = warps_in_block.by) "The product of shared tensor dimensions should equal the number of warps."

        inl index_warp = loop.proj shared.dim warps_in_block.from
        // All the warps in a block store their intermediate results into the shared tensor.
        tensor_set index_warp warp_result shared

        sync miniblock
        
        inl threads_in_warp = threads_in_warp()
        reduce_coalesced create_coalesced_threads() f (
            if threads_in_warp.from < snd shared.dim then
                tensor_index (fst index_warp, threads_in_warp.from) shared
            else
                neutral_element
            )

inl map_reduce_replicate_map_2d forall dim a b c.
        (block : ref cooperative_groups.thread_block)
        (neutral_element : b)
        (map_in : a -> b)
        (f : b -> b -> b)
        (map_out : b -> b -> c)
        (from : tensor (dim * int) a) (to : tensor (dim * int) c) : () =
    assert (from.dim = to.dim) "The input and the output tensor dimensions have to be equal."

    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups

    inl from =
        from 
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
    inl (dim_block,dim_local),sizeof_16 = from.dim
    // sizeof_16 = 4 for a f32 datatype
    // loop.prod dim_block = threads_per_block()
    // When the innermost dimension of the input is greater than the number of threads per block the `fst dim_block = 1`.
    // What loop.rigid_split does distributes the threads over the tensor. It does it in such a manner so that the elementwise product of
    // dim_block * dim_block = fst from.dim 
    // Now that the split has been done the dim_local can be iterated over using a linear loop.
    // Compared to the regular reduce this does have the disadvantage of requiring the inner dims to be powers of 2 when less than thread per block,
    // and for their size to be known at compile time.
    // But its a sacrifice I am willing to make.
    // All the tensors should have statically known sizes anyway as it will make the register use much more efficient.

    inl threads_per_miniblock = snd dim_block
    inl miniblock = create_thread_group_from_thread_block block threads_per_miniblock
    inl from =
        from
        |> curry_fst
        |> threads_in_block 
        |> apply_proj 
        |> curry_fst

    inl to =
        to 
        |> factorize_sizeof_16
        |> split_into_swapped_fst (loop.rigid_split threads_per_block())
        |> curry_fst
        |> threads_in_block 
        |> apply_proj 
        |> curry_fst

    loop.linear (fst from.dim) fun i =>
        inl from, to = apply i from, apply i to
        inl local_f = tensor_create from.dim

        // Loads the tile into the registers after mapping it.
        loop.linear (fst from.dim) fun i =>
            inl from, local_f = apply i from, apply i local_f
            inl local = tensor_create from.dim
            memcpy_sync (local, from)
            loop.linear from.dim fun i =>
                tensor_set i (map_in (tensor_index i local)) local_f
                
        // Reduces the individual thread elements. 
        inl result = loop._dup neutral_element
        loop.linear local_f.dim fun i =>
            f (tensor_index i local_f) result
            |> loop._set result

        // Reduces the blockwise results. Broadcasts the final result to all the threads.
        inl result = block_reduce_2d (threads_per_miniblock, miniblock) neutral_element f result

        // Stores the result into global memory after mapping it.
        loop.linear (fst to.dim) fun i =>
            inl to, local_f = apply i to, apply i local_f
            inl local = tensor_create to.dim
            loop.linear to.dim fun i =>
                tensor_set i (map_out (tensor_index i local_f) result) local
            memcpy_sync (to, local)

    __syncthreads()

// Reduces the innermost dimension of a given tensor.
inl reduce_2d forall dim a. neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor dim a) : () =
    assert (fst from.dim = to.dim) "The first dimension of the input has to equal the dimension of the output in the reduce_2d kernel."
    
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/reduce.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from = apply i from |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl from = apply i from
            inl l_from = tensor_create from.dim
            memcpy_sync (l_from, from)
            loop.for {from=0; nearTo=l_from.dim} (fun i => f (tensor_index i l_from)) result
            |> loop._set result

        tensor_set i (reduce_coalesced create_coalesced_threads() f result) to

    __syncthreads()

inl scan_2d_template forall dim a. is_inclusive neutral_element (f : a -> a -> a) (from : tensor (dim * int) a) (to : tensor (dim * int) a) : () = 
    assert (from.dim = to.dim) "The dimensions of the two inputs to the inclusive scan kernel need to be the same."
    
    global "#include <cooperative_groups.h>"
    global "#include <cooperative_groups/scan.h>"
    open cooperative_groups
    
    loop.projective warps_in_block(fst from.dim) fun i =>
        inl from,to = apply i from |> factorize_sizeof_16, apply i to |> factorize_sizeof_16
        inl result = loop._dup neutral_element
        loop.projective threads_in_warp(fst from.dim) fun i =>
            inl group = create_coalesced_threads()
            inl from,to = apply i from, apply i to
            inl l_from = tensor_create from.dim

            memcpy_sync (l_from, from)
            inl prefix,sum = 
                loop.for {from=0; nearTo=l_from.dim} (fun i => f (tensor_index i l_from)) neutral_element
                |> exclusive_scan_coalesced' group neutral_element f
            // semaphore.write_ln_device {i result prefix sum}
            loop.for {from=0; nearTo=l_from.dim} (fun i s => 
                if is_inclusive then
                    inl s = f (tensor_index i l_from) s
                    tensor_set i s l_from
                    s
                else
                    inl x = tensor_index i l_from
                    tensor_set i s l_from
                    f x s
                ) (f result prefix)
            |> ignore
            memcpy_sync (to, l_from)
            loop._set result (f result sum)

    __syncthreads()

// Scans the innermost dimension of a given tensor.
inl inclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d_template true
// Scans the innermost dimension of a given tensor.
inl exclusive_scan_2d forall dim a. : a -> (a -> a -> a) -> tensor (dim * int) a -> tensor (dim * int) a -> () = scan_2d_template false

inl test1() =
    inl m,n : int * int = 3, 36*4
    // inl [ma; mb; mc] = listm.map cupy.random_normal{mean=0; std=1} ([swap (m, k) a_trans; swap (k, n) b_trans; m, n])
    inl input : _ _ int = cupy.arange{from=0; nearTo=m,n; by=1}
    // inl input : _ _ int = cupy.ones (m,n)
    inl output_inclusive : _ _ int = cupy.zeros (m,n)
    inl output_exclusive : _ _ int = cupy.zeros (m,n)
    inl output_reduce : _ _ int = cupy.zeros m
    run fun _ =>
        map ((+) 1) input input
        exclusive_scan_2d 0 (+) input output_exclusive
        inclusive_scan_2d 0 (+) input output_inclusive
        reduce_2d 0 (+) input output_reduce
    printing.tensor_print_ln 1024 input
    printing.tensor_print_ln 1024 output_exclusive
    printing.tensor_print_ln 1024 output_inclusive
    printing.tensor_print_ln 1024 output_reduce
    ()

inl main() = test1()