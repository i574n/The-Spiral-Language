open corebase
open corecuda
open tensorm


inl constant() = 
    // Set this to true to verify the correctness of the GPU-computed matrix.
    inl cpu_debug = false
    // Set this to fales to use more than 64 Kb of shared memory to cache data, to
    // improve the performance of the computations on GPU.
    // Note that you need a GPU that can have more than 64 Kb of shared memory
    // per multiprocessor.
    inl shared_memory_limit_64k = false

    // MMA matrix tile dimensions.
    inl m = 16 : int
    inl n = 16 : int
    inl k = 8 : int

    // GEMM configuration.
    inl m_tiles = 512 : int
    inl n_tiles = 512 : int
    inl k_tiles = 512 : int

    inl m_global = m * m_tiles
    inl n_global = n * n_tiles
    inl k_global = k * k_tiles

    inl c_layout = "wmma::mem_col_major"
    
    // Implementation constants.
    inl warp_size = 32 : int
    inl warps_per_block = 8 : int
    inl threads_per_block = warp_size * warps_per_block
    // With only 64 Kb shared memory available, we can fit two 8-tile chunks of
    // the A and B matrix data, that is (M = 16) * (K = 8) * 8 * (CHUNK_K = 8)
    // * sizeof(float) = 32 Kb each.
    // (i.e. two 8x8 arrays of tiles of 16x8 float-typed elements per CTA).
    // But we cannot account the 8 Kb total skew overhead, without which the performance
    // would be severely impacted. So we choose to reduce the chunk size in half,
    // i.e. the amount of A and B matrix data we cache in shared memory.
    // Accordingly, this doubles the number of outer iterations across the global K
    // dimension, which only slightly impacts the performance.    
    inl chunk_k : int = if shared_memory_limit_64k then 4 else 8
    inl sizeof_float = 4
    inl sizeof_int4 = 16
    inl chunk_line_bytes = chunk_k * k * sizeof_float
    inl warp_copy_bytes = warp_size * sizeof_int4
    inl chunk_copy_lines_per_warp = warp_copy_bytes / chunk_line_bytes
    inl chunk_copy_line_lanes = warp_size / chunk_copy_lines_per_warp
    inl block_row_warps = 2 : int
    inl block_col_warps = 4 : int

    inl warp_row_tiles = 4 : int
    inl warp_col_tiles = 2 : int

    inl block_row_tiles = warp_row_tiles * block_row_warps
    inl block_col_tiles = warp_col_tiles * block_col_warps

    inl global_mem_stride = n_global

    inl shmem_stride = n * block_row_tiles
    inl shmem_offset = n * warp_row_tiles

    // The macro below is used to shift rows of the A matrix and columns of the B matrix
    // in shared memory to minimize possible bank conflicts.
    // Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
    // data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
    // is not specified for that function, each lane in the warp can read one or multiple matrix
    // elements from different matrix rows or columns.
    // For shared memory, such access can result in bank conflicts if different rows / columns
    // of the matrix map to the same bank. By shifting each row and column by a few bytes, we
    // make sure that they map to different banks, thus reducing the number of possible bank
    // conflicts.
    // The number of 8 four-byte "float" elements is chosen as the minimum possible shift because
    // we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
    inl skew_float = 8 : int
    {
    cpu_debug
    shared_memory_limit_64k
    m n k
    m_tiles n_tiles k_tiles
    m_global n_global k_global
    c_layout
    warp_size warps_per_block threads_per_block
    chunk_k chunk_line_bytes
    warp_copy_bytes chunk_copy_lines_per_warp chunk_copy_line_lanes 
    block_row_warps block_col_warps
    warp_row_tiles warp_col_tiles
    block_row_tiles block_col_tiles
    global_mem_stride
    shmem_stride
    shmem_offset
    skew_float
    }

inl simple_wmma_tf32gemm alpha (a : tensor (int * int) f32) (b : tensor (int * int) f32) beta (c : tensor (int * int) f32) =
    // TODO: we need dimensionality checking here

    inl {threads_per_block warp_size warps_per_block} = constant()
    inl a = // row_major
        reshape (fun m,k => {m=m / 16}, {m_frag=16 : int}, {k=k / 8}, {k_frag=8 : int}) a
        |> reorder (fun m,{m_frag},k,{k_frag} => m,k,(m_frag,k_frag))
    // inl b = // row_major
    //     reshape (fun k,n => {k=k / 8}, {k_frag=8 : int}, {n=n / 16}, {n_frag=16 : int}) b
    //     |> reorder (fun k,k_frag,n,n_frag => (k,n),(k_frag,n_frag))
    inl b = // col_major
        reshape (fun n,k => {n=n / 16}, {n_frag=16 : int}, {k=k / 8}, {k_frag=8 : int}) b
        |> reorder (fun n,{n_frag},k,{k_frag} => n,k,(n_frag,k_frag))
    inl c = // row_major
        reshape (fun m,n => {m=m / 16}, {m_frag=16 : int}, {n=n / 16}, {n_frag=16 : int}) c
        |> reorder (fun m,{m_frag},n,{n_frag} => (m,n),(m_frag,n_frag))
    
    // threads per block
    inl blocks = threads_per_block
    // blocks per grid (number of SMs in a RTX 4060)
    inl grids = 24
    run grids blocks (fun () =>
        global "#include <assert.h>"
        global "#include <mma.h>"
        global "#include <cooperative_groups.h>"
        global "using namespace cooperative_groups;"

        open cooperative_groups
        inl grid = create_grid()
        inl tile : _ (thread_block_tile 32 _) = create_thread_block_tile grid
        
        // Reshapes the tensor dimension into fragments.
        // For this example, we'll leave the K as the middle dimension over which the warps will iterate over.
        // In the upcoming examples, we will split the tensor dimensions more finely.

        loop_tile' tile (fst a.dim, fst b.dim) (fun m,n =>
            inl a = apply m a
            inl b = apply n b
            inl c = apply (m,n) c

            open wmma

            inl acc : fragment accumulator 16 16 8 row_major f32 = create_fragment
            fill_fragment acc 0
            loop_linear' (fst a.dim) (fun k =>
                inl a = apply k a
                inl b = apply k b

                inl a_frag : fragment matrix_a 16 16 8 row_major tf32 = create_fragment
                inl b_frag : fragment matrix_b 16 16 8 col_major tf32 = create_fragment

                load_matrix_sync_tf32 a_frag a
                load_matrix_sync_tf32 b_frag b

                mma_sync acc a_frag b_frag acc
                )

            inl c_frag : fragment accumulator 16 16 8 row_major f32 = create_fragment
            load_matrix_sync c_frag c
            loop.for' {from=0; nearTo=length c_frag} (fun i =>
                set c_frag i (alpha * index acc i + beta * index c_frag i)
                )
            store_matrix_sync c c_frag
            )
        )


inl main() =
    inl grid_range () : int = $"gridDim.x * blockDim.x"
    inl linear_id () : int = $"threadIdx.x + blockIdx.x * blockDim.x"

    inl get_body forall dim el. (x : tensor dim el) : array el = 
        real tensorm.utils.map (fun (tensor_body {array}) => array) x.bodies : array el
    inl random_normal dim =
        inl len : int = real open real_core in tensorm.utils.foldBack (*) dim 1
        inl t : array f32 = $"cp.random.normal(0,1,!len,cp.float32)" 
        fromArray t |> reshape (const dim)

    inl cp_matmul (a : tensor (int * int) float) (b : tensor (int * int) float) : tensor (int * int) float =
        inl a_dim, b_dim = a.dim, b.dim
        inl a_body,b_body : array float * array float = get_body a, get_body b
        $"cp.matmul(!a_body.reshape(!a_dim),cp.transpose(!b_body.reshape(!b_dim))).flatten()" |> fromArray |> reshape (const (fst a_dim, fst b_dim))

    inl [a; b] = listm.map random_normal ([16, 8; 16, 8] : _ (int * int))
    inl c = cp_matmul a b
    inl c' = tensor_create (16, 16)
    simple_wmma_tf32gemm 1 a b 0 c'
    inl c,c' = get_body c, get_body c'
    $"cp.max(cp.abs(!c'-!c))" : f32
    