open corebase
open corecuda
open tensorm


inl constant() = 
    // Set this to true to verify the correctness of the GPU-computed matrix.
    inl cpu_debug = false
    // Set this to fales to use more than 64 Kb of shared memory to cache data, to
    // improve the performance of the computations on GPU.
    // Note that you need a GPU that can have more than 64 Kb of shared memory
    // per multiprocessor.
    inl shared_memory_limit_64k = false

    // MMA matrix tile dimensions.
    inl m' = 16 : int
    inl n' = 16 : int
    inl k' = 8 : int

    // GEMM configuration.
    inl m_tiles = 512 : int
    inl n_tiles = 512 : int
    inl k_tiles = 512 : int

    inl m_global = m' * m_tiles
    inl n_global = n' * n_tiles
    inl k_global = k' * k_tiles

    inl c_layout = "wmma::mem_col_major"
    
    // Implementation constants.
    inl warp_size = 32 : int
    inl warps_per_block = 8 : int
    inl threads_per_block = warp_size * warps_per_block
    // With only 64 Kb shared memory available, we can fit two 8-tile chunks of
    // the A and B matrix data, that is (M = 16) * (K = 8) * 8 * (CHUNK_K = 8)
    // * sizeof(float) = 32 Kb each.
    // (i.e. two 8x8 arrays of tiles of 16x8 float-typed elements per CTA).
    // But we cannot account the 8 Kb total skew overhead, without which the performance
    // would be severely impacted. So we choose to reduce the chunk size in half,
    // i.e. the amount of A and B matrix data we cache in shared memory.
    // Accordingly, this doubles the number of outer iterations across the global K
    // dimension, which only slightly impacts the performance.    
    inl chunk_k : int = if shared_memory_limit_64k then 4 else 8
    inl sizeof_float = 4
    inl sizeof_int4 = 16
    inl chunk_line_bytes = chunk_k * k' * sizeof_float
    inl warp_copy_bytes = warp_size * sizeof_int4
    inl chunk_copy_lines_per_warp = warp_copy_bytes / chunk_line_bytes
    inl chunk_copy_line_lanes = warp_size / chunk_copy_lines_per_warp
    inl block_row_warps = 2 : int
    inl block_col_warps = 4 : int

    inl warp_row_tiles = 4 : int
    inl warp_col_tiles = 2 : int

    inl block_row_tiles = warp_row_tiles * block_row_warps
    inl block_col_tiles = warp_col_tiles * block_col_warps

    inl global_mem_stride = n_global

    inl shmem_stride = n' * block_row_tiles
    inl shmem_offset = n' * warp_row_tiles

    // The macro below is used to shift rows of the A matrix and columns of the B matrix
    // in shared memory to minimize possible bank conflicts.
    // Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
    // data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
    // is not specified for that function, each lane in the warp can read one or multiple matrix
    // elements from different matrix rows or columns.
    // For shared memory, such access can result in bank conflicts if different rows / columns
    // of the matrix map to the same bank. By shifting each row and column by a few bytes, we
    // make sure that they map to different banks, thus reducing the number of possible bank
    // conflicts.
    // The number of 8 four-byte "float" elements is chosen as the minimum possible shift because
    // we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
    inl skew_float = 8 : int
    {
    cpu_debug
    shared_memory_limit_64k
    m' n' k'
    m_tiles n_tiles k_tiles
    m_global n_global k_global
    c_layout
    warp_size warps_per_block threads_per_block
    chunk_k chunk_line_bytes
    warp_copy_bytes chunk_copy_lines_per_warp chunk_copy_line_lanes 
    block_row_warps block_col_warps
    warp_row_tiles warp_col_tiles
    block_row_tiles block_col_tiles
    global_mem_stride
    shmem_stride
    shmem_offset
    skew_float
    }

inl simple_wmma_tf32gemm alpha (a,ta : tensor (int * int) f32 * bool) (b,tb : tensor (int * int) f32 * bool) beta (c,tc : tensor (int * int) f32 * bool) =
    inl _ = 
        inl swap (a,b) x = if x then b,a else a,b
        inl a_dim = swap a.dim ta
        inl b_dim = swap b.dim tb
        inl c_dim = swap c.dim tc
        assert (snd a_dim = fst b_dim) "The K dimension of the A and B tensors must match."
        assert (fst a_dim = fst c_dim) "The M dimension of the A and C tensors must match."
        assert (snd b_dim = snd c_dim) "The N dimension of the B and C tensors must match."

    inl {threads_per_block warp_size warps_per_block m' n' k'} = constant()

    inl a = 
        if ta then // col_major
            reshape (fun k,m => {k=k / k'}, {k_frag=k' : int}, {m=m / m'}, {m_frag=m' : int}) a
            |> reorder (fun k,{k_frag},m,{m_frag} => m,k,(k_frag,m_frag))
        else // row_major
            reshape (fun m,k => {m=m / m'}, {m_frag=m' : int}, {k=k / k'}, {k_frag=k' : int}) a
            |> reorder (fun m,{m_frag},k,{k_frag} => m,k,(m_frag,k_frag))
    inl b =
        if tb then // col_major
            reshape (fun n,k => {n=n / n'}, {n_frag=n' : int}, {k=k / k'}, {k_frag=k' : int}) b
            |> reorder (fun n,{n_frag},k,{k_frag} => n,k,(n_frag,k_frag))
        else // row_major
            reshape (fun k,n => {k=k / k'}, {k_frag=k' : int}, {n=n / n'}, {n_frag=n' : int}) b
            |> reorder (fun k,{k_frag},n,{n_frag} => n,k,(k_frag,n_frag))
    inl c =
        if tc then // col_major
            reshape (fun n,m => {n=n / n'}, {n_frag=n' : int}, {m=m / m'}, {m_frag=m' : int}) c
            |> reorder (fun n,{n_frag},m,{m_frag} => (m,n),(n_frag,m_frag))
        else // row_major
            reshape (fun m,n => {m=m / m'}, {m_frag=m' : int}, {n=n / n'}, {n_frag=n' : int}) c
            |> reorder (fun m,{m_frag},n,{n_frag} => (m,n),(m_frag,n_frag))
    
    // threads per block
    inl blocks = threads_per_block
    // blocks per grid (number of SMs in a RTX 4060)
    inl grids = 24
    run grids blocks (fun () =>
        global "#include <mma.h>"
        global "using namespace nvcuda;"
        global "#include <cooperative_groups.h>"
        global "using namespace cooperative_groups;"

        open cooperative_groups
        
        // Reshapes the tensor dimension into fragments.
        // For this example, we'll leave the K as the middle dimension over which the warps will iterate over.
        // In the upcoming examples, we will split the tensor dimensions more finely.

        loop_warps_in_grid' (fst a.dim, fst b.dim) (fun m,n =>
            inl a = apply m a
            inl b = apply n b
            inl c = apply (m,n) c

            open wmma

            inl acc : fragment' accumulator 16 16 8 f32 = create_fragment' tc
            fill_fragment' acc 0
            loop_linear' (fst a.dim) (fun k =>
                inl a = apply k a
                inl b = apply k b

                inl a_frag : fragment' matrix_a 16 16 8 tf32 = create_fragment' ta
                inl b_frag : fragment' matrix_b 16 16 8 tf32 = create_fragment' tb

                load_matrix_sync_tf32' a_frag a
                load_matrix_sync_tf32' b_frag b

                mma_sync' acc a_frag b_frag acc
                )

            inl c_frag : fragment' accumulator 16 16 8 f32 = create_fragment' tc
            load_matrix_sync' c_frag c
            loop.linear' (length c_frag) (fun i =>
                set c_frag i (alpha * index acc i + beta * index c_frag i)
                )
            store_matrix_sync' c c_frag
            )
        )


inl main() =
    inl grid_range () : int = $"gridDim.x * blockDim.x"
    inl linear_id () : int = $"threadIdx.x + blockIdx.x * blockDim.x"

    inl get_body forall dim el. (x : tensor dim el) : array el = 
        real tensorm.utils.map (fun (tensor_body {array}) => array) x.bodies : array el

    inl random_normal dim =
        inl len : int = real open real_core in tensorm.utils.foldBack (*) dim 1
        inl t : array f32 = $"cp.random.normal(0,1,!len,cp.float32)" 
        fromArray t |> reshape (const dim)

    inl swap (a,b) x = if x then b,a else a,b
    inl cp_matmul (a : tensor (int * int) float * bool) (b : tensor (int * int) float * bool) tc : tensor (int * int) float =
        inl f (ta : bool) (a_body : array float) (a_dim : int * int) : array float = 
            inl x : array float = $"!a_body.reshape(!a_dim)"
            if ta then $"cp.transpose(!x)" else x
        inl g (a,ta : tensor (int * int) float * bool) = f ta (get_body a) a.dim
        inl a_body,b_body : array float * array float = g a, g b
        
        inl a_dim = swap (fst a).dim (snd a)
        inl b_dim = swap (fst b).dim (snd b)
        inl c_dim = fst a_dim, snd b_dim
        f tc $"cp.matmul(!a_body,!b_body)" c_dim 
        |> fun x => $"!x.flatten()"
        |> fromArray |> reshape (const (swap c_dim tc))

    inl m,n,k : int * int * int = 256, 256, 128
    inl ta,tb,tc = true,false,true
    inl [a; b] = listm.map random_normal ([swap (m, k) ta; swap (k, n) tb])
    inl c = cp_matmul (a,ta) (b,tb) tc
    inl c' = tensor_create (swap (m, n) tc)
    simple_wmma_tf32gemm 1 (a, ta) (b, tb) 0 (c', tc)
    inl c,c' = get_body c, get_body c'
    $"cp.max(cp.abs(!c'-!c))" : f32