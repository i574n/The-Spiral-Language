// A CFR+ agent with linear weightings.

// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in am.mapInplace (fun _ _ => v) temp
    else am.mapInplace (fun _ x => x / normalizing_sum) temp
    temp

type state key act = dictm.dict key (mut {regret : a u64 f64; utils : a u64 f64; avg_policy : a u64 f64; actions : a u64 act; t : u64})
inl value (d : state _ _) actions = dictm.memoize d fun _ => 
    inl Empty = am.init (length actions) (fun _ => 0)
    mut {regret = Empty; utils = Empty; avg_policy = Empty; actions; t = 1}

inl terminal _ = ()

open nodes
inl funsPlay (d : state _ _) = player_funs {
    terminal action = fun {player actions next} =>
        inl num_actions = length actions
        inl policy = regret_match (value d actions player.observations).avg_policy
        inl i = $"numpy.random.choice(!num_actions,p=!policy)"
        inl p,a = index policy i, index actions i
        next (log_probm.from' p,a)
    }

union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _ _) = player_funs {
    terminal action = fun {chance_prob player player' id actions next} =>
        inl d = value d actions player.observations
        inl op_prob = (log_probm.exp (log_probm.add chance_prob player')).policy

        inl next prob a = 
            if prob = 0 && op_prob = 0 then r2 0 // the pruning optimization
            else next (log_probm.from' prob,a)

        inl weighted_sum policy = am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) policy actions

        match mode with
        | PlayAvgPolicy => weighted_sum (regret_match d.avg_policy)
        | PlayCurrentPolicy => weighted_sum (regret_match d.regret)
        | TrainCurrentPolicy =>
            inl reward,reward_wsum =
                am.mapFold2 (fun s prob a => 
                    inl r = next prob a
                    r, s +. r *. prob
                    ) (r2 0) (regret_match d.regret) actions
            inl ~id = id
            inl t = f64 d.t
            d.utils |> am.mapInplace (fun i x => x + op_prob * r2_index (index reward i) id)
            reward_wsum
    }

inl update (d : state _ _) =
    d |> dictm.fold (fun () (k,v) => // TODO: Fix the type inference bug here
        inl v : mut {actions : a u64 _; avg_policy : a u64 f64; regret : a u64 f64; t : u64; utils : a u64 f64} = v
        inl policy = regret_match v.regret
        inl us = am.fold2 (fun s prob u => s + prob * u) 0 policy v.utils
        v.regret |> am.mapInplace (fun i x => max 0 <| x + f64 v.t * (index v.utils i - us))
        inl policy = regret_match v.regret
        v.avg_policy |> am.mapInplace (fun i x => x + f64 v.t * index policy i)
        v.t <- v.t+1
        ) ()