// A MC CFR agent.

// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in am.mapInplace (fun _ _ => v) temp
    else am.mapInplace (fun _ x => x / normalizing_sum) temp
    temp
    
type state key key2 act = dictm.dict key ({regret : a u64 f64; avg_policy : a u64 f64; qs : a u64 wval; actions : a u64 act})
inl value (d : state _ _ _) actions = dictm.memoize d fun _ => 
    inl Empty = am.init (length actions) (fun _ => 0)
    {regret = Empty; avg_policy = Empty; qs = am.init (length actions) (fun _ => wval (r2 0) 0); actions}

inl terminal _ = ()

open nodes
inl funsPlay (d : state _ _ _) = player_funs {
    terminal action = fun {player actions next} =>
        inl num_actions = length actions
        inl policy = regret_match (value d actions player.observations).avg_policy
        inl i = $"numpy.random.choice(!num_actions,p=!policy)"
        inl p,a = index policy i, index actions i
        next (log_prob_from_sample p,a)
    }

inl Epsilon = 2 ** -2
inl Alpha = 2 ** -5
union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _ _ _) = player_funs {
    terminal action = fun ~{chance_prob player player'=op_prob id actions next} => join
        inl key = player.observations
        inl d = value d actions key
        inl num_actions = length actions
        inl sample policy =
            inl i = $"numpy.random.choice(!num_actions,p=!policy)"
            next (log_prob_from_sample (index policy i),index actions i)

        match mode with
        | PlayAvgPolicy => sample (regret_match d.avg_policy)
        | PlayCurrentPolicy => sample (regret_match d.regret)
        | TrainCurrentPolicy =>
            inl policy = regret_match d.regret
            inl sample_policy = am.map (fun x => Epsilon * (1 / f64 num_actions) + (1 - Epsilon) * x) policy
            inl i' : u64 = $"numpy.random.choice(!num_actions,p=!sample_policy)"
            inl ({sample} as p) = {sample=index sample_policy i'; policy=index policy i'}
            inl r = next (log_prob_from p,index actions i')
            inl qs = d.qs
            inl (u : a _ _),us =
                am.initFold num_actions (fun s i =>
                    inl q = ,(index qs i)
                    inl r = if i = i' then (r -! q) /! sample +! q else q 
                    r, s +! r *! index policy i
                    ) (r2 0)
            inl self_prob = player.prob
            inl env_prob = chance_prob +@ op_prob
            inl regret_prob = exp ((0 - self_prob.sample) + (env_prob.policy - env_prob.sample))
            set qs i' (index qs i' *, (1 - Alpha) +, wval r regret_prob *, Alpha)
            d.regret |> am.mapInplace (fun i x => max 0 <| x + regret_prob * !!(index u i -! us) id)
            inl policy = regret_match d.regret
            inl avg_policy_prob = exp ((self_prob.policy - self_prob.sample) - (chance_prob.sample + op_prob.sample))
            d.avg_policy |> am.mapInplace (fun i x => x + avg_policy_prob * index policy i)
            us
    }

inl funsTest (d : state _ _ _) = player_funs {
    terminal action = fun {player player' actions next} =>
        inl d = value d actions player.observations
        inl next prob a = 
            if prob = 0 && (player'.policy - player'.sample) = -inf then r2 0
            else next (log_prob_from_policy prob,a)

        am.fold2 (fun s prob a => s +! next prob a *! prob) (r2 0) (regret_match d.avg_policy) actions
    }