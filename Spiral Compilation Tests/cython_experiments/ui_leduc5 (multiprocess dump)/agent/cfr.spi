// A CFR agent.

// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in am.mapInplace (fun _ _ => v) temp
    else am.mapInplace (fun _ x => x / normalizing_sum) temp
    temp

type state key act = dictm.dict key {regret : a u64 f64; avg_policy : a u64 f64; actions : a u64 act}
inl value (d : state _ _) actions = dictm.memoize d fun _ => 
    inl Empty = am.init (length actions) (fun _ => 0)
    {regret = Empty; avg_policy = Empty; actions}

inl terminal _ = ()

open nodes
inl funsPlay (d : state _ _) = player_funs {
    terminal action = dyn fun {player actions next} =>
        inl num_actions = length actions
        inl actions_prob = regret_match (value d actions player.observations).avg_policy
        inl i = $"np.random.choice(!num_actions,p=!actions_prob)"
        inl p,a = index actions_prob i, index actions i
        next ((log_probm.from p,a),state player)
    }

union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _ _) = player_funs {
    terminal action = dyn fun {chance_prob player player' id actions next} =>
        inl d = value d actions player.observations
        inl op_prob = log_probm.exp (log_probm.add chance_prob player')
        inl state = state player

        inl next prob a = 
            if prob = 0 && op_prob = 0 then r2 0 // the pruning optimization
            else next ((log_probm.from prob,a),state)

        let weighted_sum actions_prob = am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) actions_prob actions

        match mode with
        // For evaling the overall quality of the policy whose agents are done training.
        | PlayAvgPolicy => weighted_sum (regret_match d.avg_policy)
        // Alternating updates between the players works better than training both simultaneously.
        // For the sake of training it is better to play the latest policy and get the freshest data instead of 
        // sampling the old ones, which in effect would happen if the avg_policy was used for training.
        | PlayCurrentPolicy => weighted_sum (regret_match d.regret)
        // Trains the current policy. The old F# version in the `v0.2` branch of the Spiral repo has some bugs.
        // The way it is done currently reflects my current understanding of the way the algorithm is supposed to be implemented.
        // For example, I am decently sure that `self_prob` should not have the chance probability like the paper suggests.
        // Also the latest policy should be added to the average one rather than the stale one.
        // To speed up learning I also reweight the rewards using the updated policy.
        | TrainCurrentPolicy =>
            inl reward,reward_wsum =
                am.mapFold2 (fun s prob a => 
                    inl r = r2_index (next prob a ) id
                    r, s + prob * r
                    ) 0 (regret_match d.regret) actions
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = regret_match d.regret
            inl self_prob = log_probm.exp player.prob
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
            weighted_sum actions_prob
    }