// A MC CFR agent.

// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in am.mapInplace (fun _ _ => v) temp
    else am.mapInplace (fun _ x => x / normalizing_sum) temp
    temp

type state key act = dictm.dict key ({regret : a u64 f64; avg_policy : a u64 f64; qs : a u64 f64; actions : a u64 act})
inl value (d : state _ _) actions = dictm.memoize d fun _ => 
    inl Empty = am.init (length actions) (fun _ => 0)
    {regret = Empty; avg_policy = Empty; qs = Empty; actions}

inl terminal _ = ()

open nodes
inl funsPlay (d : state _ _) = player_funs {
    terminal action = fun {player actions next} =>
        inl num_actions = length actions
        inl policy = regret_match (value d actions player.observations).avg_policy
        inl i = $"numpy.random.choice(!num_actions,p=!policy)"
        inl p,a = index policy i, index actions i
        next (log_probm.from' p,a)
    }

inl Epsilon = 2 ** -2
inl Alpha = 2 ** -8
union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _ _) = player_funs {
    terminal action = fun ~{chance_prob game_state=_,{card=op_card id=_ pot},_ player player' id actions next} => join
        inl d = value d actions player.observations
        inl num_actions = length actions
        inl sample policy =
            inl i = $"numpy.random.choice(!num_actions,p=!policy)"
            next (log_probm.from' (index policy i),index actions i)

        match mode with
        | PlayAvgPolicy => sample (regret_match d.avg_policy)
        | PlayCurrentPolicy => sample (regret_match d.regret)
        | TrainCurrentPolicy =>
            inl policy = regret_match d.regret
            inl sample_policy = am.map (fun x => Epsilon * (1 / f64 num_actions) + (1 - Epsilon) * x) policy
            inl i : u64 = $"numpy.random.choice(!num_actions,p=!sample_policy)"
            inl ({sampled} as p) = {sampled=index sample_policy i; policy=index policy i}
            inl reward = r2_index (next (log_probm.from p,index actions i)) id
            inl u,us =
                am.initFold num_actions (fun s i' =>
                    inl q = index d.qs i'
                    inl r = if i = i' then (reward - q) / sampled + q else q
                    r, s + r * index policy i
                    ) 0
            set d.qs i ((1 - Alpha) * index d.qs i + Alpha * reward)
            inl self_prob = player.prob
            inl op_prob = player'
            inl regret_prob = exp (-(self_prob.sampled) + (chance_prob.policy - chance_prob.sampled) + (op_prob.policy - op_prob.sampled))
            d.regret |> am.mapInplace (fun i x => max 0 <| x + regret_prob * (index u i - us))
            inl policy = regret_match d.regret
            inl avg_policy_prob = exp ((self_prob.policy - self_prob.sampled) - (chance_prob.sampled + op_prob.sampled))
            d.avg_policy |> am.mapInplace (fun i x => x + avg_policy_prob * index policy i)
            inl r = am.fold2 (fun s prob u => s + u * prob) 0 policy u
            if id = 0 then r2 r else r2 -r
    }

inl funsTest (d : state _ _) = player_funs {
    terminal action = fun {player actions next} =>
        inl d = value d actions player.observations
        inl next prob a = 
            if prob = 0 then r2 0
            else next (log_probm.from' prob,a)

        am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) (regret_match d.avg_policy) actions
    }