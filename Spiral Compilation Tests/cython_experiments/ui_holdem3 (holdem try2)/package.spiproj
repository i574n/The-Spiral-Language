// 6/30/2021: Things are not going well. I did a 20h training run with the architecture that worked well for me on Flop poker,
// but on Holdem I just get an aggrodonk. This is an indication that the variance of Holdem is too high for the value function
// to learn how to read hands. And this is despite me messing up and training with 10x smaller stacks that I intended to.

// Right now I am in the same situation as when I finished Leduc, tried Holdem and found it to not work. I managed to get it to
// work on Flop thanks to the many changes and thought it might work on Holdem, but it feels out of reach again.

// This is not going to work. I have no choice but to ramp up my attempt and bring in unsupervised learning. Maybe it would learn
// something if I let it train for 10 days instead of 1, but I can't risk it. Therefore, I have to have some measure of learning
// separate from reward based optimization that I am doing here.

// Update(7/1/2021): Actually, before I rashly move to transformers, I need to investigate the variance of the CFR updates.
// The reason why it is not training is probably not because of MLPs, but because CFR's reweighting does not mix with gradient
// based optimization.

// First I'll check out how the value function optimizes against the callbot and do ablation from there.

// Success. Removing the CFR reweighting from the body significantly improves the performance of the value net.
// And keeping it in the head helps as well. But rather than getting rid of it completely, it might be better to make
// the weighting on-policy. Let me copy this project and I will make some drastic changes.

packages: |core2-
modules:
    types-
    conv-
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    train
    hand_scorer
    hu_holdem
    leduc
    agent/
        uniform
        holdem_callbot
        tabular
        neural_ff_leduc
        neural_ff_holdem
    create_args_leduc
    create_args_holdem