// The fight resumes. I am giving up on deep RL, but not on making the agents. Mapping rewards to actions is a very recent
// invention, and I need to research it. I won't use it directly like in the `Training Agents using Upside-Down Reinforcement 
// Learning` paper, but I will instead make it a source of self supervised learning.

// There are too many things that need doing, so before I move onto the main thing, I need to do some ablation studies.
// First, how does replacing the importance sampling version of the value update compare with reward substitution. (Edit: Badly.)

// Update: It seems along the way I lost the ability to train on Flop. Whatever changes I introduced, some of them must have
// fundamentally broken everything. Not only do I have to track this problem down, but I also need to consider that this might
// be the reason why Holdem is not training.

// Update: That problem was using the InfNormSGD optimizer by accident. I took the chance to test it thoroughly and found it 
// performed very poorly compared to SignSGD vs the Flop callbot. I thought it might be the case that it could make more precise 
// modifications and get better performance, but if it does not work against the call bot I shouldn't expect anything from it 
// anywhere else.

packages: |core2-
modules:
    types-
    conv-
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    train
    hand_scorer
    hu_holdem
    leduc
    agent/
        uniform
        holdem_callbot
        tabular
        neural_ff_leduc
        neural_ff_holdem
    create_args_leduc_
    create_args_holdem