inl vs_human game human_pid p =
    let rec loop = function
        | Terminal: _ as g => g
        | Action: player_state,game_state,pid,actions,next as g => 
            if pid = human_pid then g
            else
                inl cs,_ = p (am.singleton (player_state,game_state,pid,actions))
                loop (next (index cs 0))
    loop game

inl vs_self game (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl data : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions,next => 
                rm.add actions_indices i
                rm.add data (player_state,game_state,pid,actions)
                rm.add nexts next
            | Terminal: x => 
                rm.add rewards (i, x)
        inl rewards_actions : a _ _ =
            if 0 < length data then
                inl (cs : a _ _),update = p data
                am.generic.map2 (<|) nexts cs |> loop |> update
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game () pl2_init)

inl vs_one game (batch_size, p1, p2) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl p1_data : ra u64 _ = am.empty
        inl p2_data : ra u64 _ = am.empty
        inl p1_nexts : ra u64 _ = am.empty
        inl p2_nexts : ra u64 _ = am.empty
        inl pids : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions,next =>
                rm.add actions_indices i
                if pid = 0 then
                    rm.add p1_data (player_state,game_state,pid,actions)
                    rm.add p1_nexts next
                else
                    rm.add p2_data (player_state,game_state,pid,actions)
                    rm.add p2_nexts next
                rm.add pids pid
            | Terminal: x =>
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length pids then
                inl p1_cs,p1_update : a _ _ * (a u64 r2 -> a u64 r2) = p1 p1_data
                inl p2_cs,p2_update : a _ _ * (a u64 r2 -> a u64 r2) = p2 p2_data
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl len = length p1_cs
                inl slice_from forall d t. (x : a d t) (i : d) : a d t = $"!x[!i:]"
                inl slice_near_to forall d t. (x : a d t) (i : d) : a d t = $"!x[:!i]"
                inl p1_rs = p1_update (slice_near_to rs len)
                inl p2_rs = p2_update (slice_from rs len)
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game () pl2_init)

type a2 x = $"numpy.ndarray[`x,ndim=2]"
type a3 x = $"numpy.ndarray[`x,ndim=3]"

// The neural model is intended to be created on the Python side and partially applied as the `policy_value_action_eval` argument. 
// `extractor` and `schema` are applied on the Spiral side before being passed as a function. 
inl neural_handler extractor (schema : schema _ _ _) policy_value_action_eval (data : ra _ (_ * _ * u8 * a st _)) =
    if length data = 0 then am.empty, id else
    !!!!Import("torch")
    inl policy_size = serialization.dense.array.size schema.policy
    inl value_size = serialization.dense.array.size schema.value
    inl action_size = serialization.sparse.int.size schema.action
    inl batch_size = length data
    inl policy_seq_size, future_seq_size = 
        am.fold (fun (s1,s2) (player_state,game_state,pid,actions) => 
            inl a,b = extractor (player_state,game_state,pid)
            max s1 (listm.length a), max s2 (listm.length b)
            ) (0u64, 0u64) data
    inl value_seq_size = policy_seq_size + future_seq_size
    inl policy_data : a3 f32 = $"numpy.zeros((!batch_size,!policy_seq_size,!policy_size),dtype=numpy.float32)"
    inl policy_mask : a2 i8 = $"numpy.zeros((!batch_size,!policy_seq_size),dtype=numpy.int8)"
    inl value_data : a3 f32 = $"numpy.zeros((!batch_size,!value_seq_size,!value_size),dtype=numpy.float32)"
    inl value_mask : a2 i8 = $"numpy.zeros((!batch_size,!value_seq_size),dtype=numpy.int8)"
    inl action_mask : a2 i8 = $"numpy.ones((!batch_size,!action_size),dtype=numpy.int8)"
    data |> am.iteri fun i_batch (player_state,game_state,pid,actions) =>
        inl present, future = extractor (player_state,game_state,pid)
        inl i_seq = 
            listm.foldBack (fun x i_seq =>
                inl f forall t. (x : a3 t) : a st t = $"!x[!i_batch,!i_seq,:]"
                schema.policy.pickle x (0, f policy_data)
                schema.value.pickle (C1of2: x) (0, f value_data)
                i_seq + 1
                ) present 0u64
        loop.for' (from:i_seq nearTo: policy_seq_size) fun i_seq => $"!policy_mask[!i_batch,!i_seq] = 1"
        inl i_seq = 
            listm.fold (fun i_seq x =>
                inl f forall t. (x : a3 t) : a st t = $"!x[!i_batch,!i_seq,:]"
                schema.value.pickle (C2of2: x) (0, f value_data)
                i_seq + 1
                ) i_seq future
        loop.for' (from:i_seq nearTo: value_seq_size) fun i_seq => $"!value_mask[!i_batch,!i_seq] = 1"
        actions |> am.iter (schema.action.pickle >> fun x => $"!action_mask[!i_batch,!x] = 0")

    inl from_numpy x = $"torch.from_numpy(!x)" : obj
    inl from_numpy_as_bool x = from_numpy ($"!x.view('bool')" : obj)
    inl action_probs, epsilon, sample_indices, update : a2 f32 * f32 * a u64 i64 * (obj -> a u64 f32) = 
        inl x : obj = policy_value_action_eval (from_numpy policy_data, from_numpy_as_bool policy_mask, from_numpy value_data, from_numpy_as_bool value_mask, from_numpy_as_bool action_mask)
        $"!x[0]", $"!x[1]", $"!x[2]", $"!x[3]"
    inl cs : a u64 _ = am.init batch_size fun b =>
        inl a = index sample_indices b
        inl policy = $"!action_probs[!b,!a]"
        inl num_actions_at_index = index data b |> fun _,_,_,actions => length actions |> f32
        inl sample = epsilon / num_actions_at_index + (1 - epsilon) * policy
        log_prob_from {policy sample}, schema.action.unpickle (conv_int a)
    
    cs, fun (rewards' : a u64 r2) : a u64 r2 =>
        inl rewards_and_env_probs : a _ _ = create (2*batch_size)
        am.generic.iteri2 (fun num_action reward (player_state,game_state,pid,actions) =>
            inl reward = !!reward pid
            set rewards_and_env_probs num_action reward

            inl regret_prob =
                inl prob = pl2_probs player_state pid
                inl prob_env = prob.chance +@ prob.op
                exp (-prob.self.sample + (prob_env.policy - prob_env.sample))
            set rewards_and_env_probs (batch_size + num_action) regret_prob
            ) rewards' data
        inl rewards = update (from_numpy rewards_and_env_probs)
        am.generic.map2 (fun a (_,_,pid,_) => if pid = 0 then r2 a else r2 -a) rewards data

inl neural extractor schema =
    inl handler = neural_handler extractor schema
    inl size =
        inl policy = serialization.dense.array.size schema.policy
        inl value = serialization.dense.array.size schema.value
        inl action = serialization.sparse.int.size schema.action
        namedtuple "Size" {action policy value}
    namedtuple "Neural" {handler size}