// A CFR+ agent with linear weightings.

// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in am.mapInplace (fun _ _ => v) temp
    else am.mapInplace (fun _ x => x / normalizing_sum) temp
    temp

type state key act = dictm.dict key (mut {regret : a u64 f64; avg_policy : a u64 f64; actions : a u64 act; t : u64})
inl value (d : state _ _) actions = dictm.memoize d fun _ => 
    inl Empty = am.init (length actions) (fun _ => 0)
    mut {regret = Empty; avg_policy = Empty; actions; t = 1}

inl terminal _ = ()

open nodes
inl funsPlay (d : state _ _) = player_funs {
    terminal action = fun {player actions next} =>
        inl num_actions = length actions
        inl policy = regret_match (value d actions player.observations).avg_policy
        inl i = $"numpy.random.choice(!num_actions,p=!policy)"
        inl p,a = index policy i, index actions i
        next (log_probm.from' p,a)
    }

union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _ _) = player_funs {
    terminal action = fun {chance_prob player player' id actions next} =>
        inl d = value d actions player.observations
        inl op_prob = (log_probm.exp (log_probm.add chance_prob player')).policy

        inl next prob a = 
            if prob = 0 && op_prob = 0 then r2 0 // the pruning optimization
            else next (log_probm.from' prob,a)

        inl weighted_sum policy = am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) policy actions

        match mode with
        | PlayAvgPolicy => weighted_sum (regret_match d.avg_policy)
        | PlayCurrentPolicy => weighted_sum (regret_match d.regret)
        | TrainCurrentPolicy =>
            inl reward,reward_wsum =
                am.mapFold2 (fun s prob a => 
                    inl r = next prob a
                    r, s +. r *. prob
                    ) (r2 0) (regret_match d.regret) actions
            inl ~id = id
            join
                inl t = f64 d.t
                d.regret |> am.mapInplace (fun i x => max 0 <| x + t * op_prob * (r2_index (index reward i) id - r2_index reward_wsum id))
                inl policy = regret_match d.regret
                inl self_prob = (log_probm.exp player.prob).policy
                d.avg_policy |> am.mapInplace (fun i x => x + t * self_prob * index policy i)
                d.t <- d.t+1
                am.fold2 (fun s prob reward => s +. reward *. prob) (r2 0) policy reward
    }