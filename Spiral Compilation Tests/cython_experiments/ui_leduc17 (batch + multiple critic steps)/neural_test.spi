inl main () =
    !!!!Import("torch")
    !!!!Import("torch.distributions")
    !!!!Import("torch.optim")
    inl game = leduc.game()
    inl net = agent.neural.create_small_leduc_net()
    inl policy,value = agent.neural.models net
    inl lr : f32 = 2 ** -14 // Note: Be careful of Cython integer power bug. `integer ** negative_integer == 0` in raw Cython.
    $"print('The learning rate is 2 **',torch.log2(torch.scalar_tensor(!lr)).item())"
    inl opt : obj = $"torch.optim.SGD([{'params':!policy.parameters()},{'params':!value.parameters()}],lr=!lr)"
    loopw.for' (from: 0i32 nearTo: 20) fun _ =>
        $"!policy.eval()" . $"!value.train()"
        loopw.for' (from: 0i32 nearTo: 5) fun _ =>
            inl r = train.vs_self 1024 (agent.neural.policy (2 ** -2) net) game
            $"print(numpy.sqrt(numpy.sum(numpy.square(!r))))"
            $"!opt.step()" . $"!opt.zero_grad(True)"
        $"print('***')"
        $"!policy.train()" . $"!value.eval()"
        inl r = train.vs_self 1024 (agent.neural.policy (2 ** -2) net) game
        $"!opt.step()" . $"!opt.zero_grad(True)"
        $"print('The mean reward is',numpy.mean(!r))"
    $"print('---')"
    $"!policy.eval()" . $"!value.eval()"
    inl r = train.vs_one 10240 agent.uniform.policy (agent.neural.policy 0 net) game
    $"print(numpy.mean(!r))"
    ()