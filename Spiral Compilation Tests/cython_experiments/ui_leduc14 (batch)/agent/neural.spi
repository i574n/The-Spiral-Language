nominal schema present future actions = {
    policy : serialization.dense.array.pu present
    value : serialization.dense.array.pu (present * future)
    actions : serialization.sparse.int.pu actions
    }
open leduc
nominal leduc_policy_input = {pid : u64; pot : u64; seq : a u64 (card * a u64 action)}
nominal leduc_net = {policy : obj; schema : schema leduc_policy_input card action; value : obj}

let obs_to_array (l : list (observation card action)) : a u64 (card * a u64 action) =
    let rec action (c,l) = function
        | Cons: (Observation: x), x' => (c,listm.rev l |> listm.toArray) :: action (x,Nil) x'
        | Cons: (Action: x), x' => action (c,x :: l) x'
        | Nil => (c,listm.rev l |> listm.toArray) :: Nil
    match listm.rev l with
    | Cons: (Observation: x), x' => action (x,Nil) x' |> listm.toArray
    | _ => failwith "Expected a card."
    
inl create_small_leduc_net () =
    inl schema =
        inl policy, value =
            open serialization.dense.array
            inl card = alt {king=Unit; queen=Unit; jack=Unit} : pu card
            inl action = alt {fold=Unit; call=Unit; raise=Unit} : pu action
            // Max pot size is 13, and 0 is impossible for Leduc due to the antes. 
            // Here I am mapping the 1-13 to 0-12 range and back.
            inl pot = wrap (in:(fun x => x-1) out:(+) 1) (int 13) 
            inl pid = int 2
            inl policy =
                wrap (in: fun (leduc_policy_input {pid pot seq}) => pid, pot, seq
                      out: fun pid,pot,seq => leduc_policy_input {pid pot seq})
                    (pid ** pot ** array 2 (card ** array 4 action))
            policy, policy ** card
        schema { 
            policy value
            actions =
                open serialization.sparse.int
                alt {fold=Unit; call=Unit; raise=Unit} : pu action
            }

    open serialization
    inl c_policy = dense.array.size schema.policy
    inl c_value = dense.array.size schema.value
    inl b,a = 64u64, sparse.int.size schema.actions

    !!!!Import("nets")
    leduc_net {schema policy = $"nets.small(!c_policy,!b,!a)" : obj; value = $"nets.small(!c_value,!b,!a)" : obj}

inl models (leduc_net {policy value schema}) = policy, value

inl policy (epsilon : f32) (leduc_net {policy value schema}) (l : ra _ _) =
    inl len = length l
    inl policy_size = serialization.dense.array.size schema.policy
    inl value_size = serialization.dense.array.size schema.value
    inl actions_size = serialization.sparse.int.size schema.actions
    inl data_value : obj = $"torch.zeros(!len,!value_size)"
    l |> am.iteri fun b (player_state, (leduc_state (p1,p2,community_card)), pid, actions) =>
        inl seq = pl2_observations player_state pid |> obs_to_array
        inl pot = if pid = p1.id then p1.pot else p2.pot
        schema.value.pickle (leduc_policy_input {pid= $"!pid"; pot= $"!pot"; seq},if pid = p1.id then p2.card else p1.card) (0, $"!data_value[!b,:].numpy()")
    inl action_indices : a u64 (a u64 _) = l |> am.generic.map fun (_,_,_,actions) => am.map schema.actions.pickle actions
    inl policy_raw : obj = $"!policy.forward(!data_value[:,:!policy_size])"
    inl policy_mask : obj = $"torch.full((!len,!actions_size),float('-inf'))"
    am.iteri (fun b => am.iter (fun a => $"!policy_mask[!b,!a] = 0")) action_indices
    inl policy_log_probs : obj = $"torch.log_softmax(!policy_mask + !policy_raw.cpu(),dim=-1)"
    inl policy_probs : obj = $"torch.exp(!policy_log_probs.detach())"

    inl sample_probs : obj = 
        if epsilon = 0 then policy_probs else
        inl sample_probs : obj = $"torch.empty_like(!policy_probs)"
        am.iteri (fun b l =>
            inl valid_size = f32 (length l)
            loop.for' (from:0 nearTo: actions_size) fun a =>
                inl x = if $"!policy_mask[!b,!a]" = 0f32 then epsilon / valid_size + (1.0 - epsilon) * $"!policy_probs[!b,!a]" else 0
                $"!sample_probs[!b,!a] = !x"
            ) action_indices
        sample_probs
    inl sample_indices : a u64 i64 = $"torch.distributions.Categorical(!sample_probs).sample().numpy()"

    inl l' : a _ _ =
        am.mapi (fun b a =>
            inl policy : f32 = $"!policy_probs[!b,!a]"
            inl sample : f32 = $"!sample_probs[!b,!a]"
            log_prob_from {policy sample}, schema.actions.unpickle a
            ) sample_indices
    l', fun (r : a u64 r2) =>
        if length r = 0 then r else
        inl value : obj = $"!value.forward(!data_value).cpu()"
        inl value_sampled : obj = $"!value.detach().clone()"
        inl value_signal : obj = $"torch.zeros_like(!value)"
        am.iteri2 (fun b a (r2 r) =>
            inl sample : f32 = $"!sample_probs[!b,!a]"
            inl q : f32 = $"!value_sampled[!b,!a]"
            inl dif : f32 = $"!r - !q"
            $"!value_signal[!b,!a] -= !dif"
            $"!value_sampled[!b,!a] += !dif / !sample"
            ) sample_indices r
        $"print(torch.mean(!value_signal))"
        $"!value.backward(!value_signal)"

        inl means : obj = $"torch.einsum('ab,ab->a',!value_sampled,!policy_probs)"
        inl pids : obj = am.generic.map (fun _,_,pid,_ => if pid = 0 then 1 else -1f32) l |> fun (x : a _ _) => $"torch.from_numpy(!x)"
        $"!policy_log_probs.backward(!pids.unsqueeze(-1) * (!means.unsqueeze(-1) - !value_sampled))"
        $"!means.numpy()" : a u64 r2