inl main () =
    !!!!Import("collections")
    open leduc
    inl vs_self : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 = train.vs_self leduc.game()
    inl vs_one : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 = train.vs_one leduc.game()
    inl schema = agent.neural.leduc_schema()
    inl handler : _ -> a u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) = train.neural_handler agent.neural.leduc_extractor schema
    inl size = 
        inl policy = serialization.dense.array.size schema.policy
        inl value = serialization.dense.array.size schema.value
        inl action = serialization.sparse.int.size schema.action
        namedtuple "Size" {action policy value}
    inl neural = namedtuple "Neural" {handler size}
    inl uniform_player : a u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) = agent.uniform.policy
    record {vs_self vs_one neural uniform_player}