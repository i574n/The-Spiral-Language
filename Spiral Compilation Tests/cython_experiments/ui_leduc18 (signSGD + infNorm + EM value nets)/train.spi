inl vs_self game (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl actions : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions',next => 
                rm.add actions_indices i
                rm.add actions (player_state,game_state,pid,actions')
                rm.add nexts next
            | Terminal: x => 
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length actions then
                inl cs,update = p actions
                am.generic.map2 (<|) nexts cs |> loop |> update
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl vs_one game (batch_size, p1, p2) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl p1_actions : ra u64 _ = am.empty
        inl p2_actions : ra u64 _ = am.empty
        inl p1_nexts : ra u64 _ = am.empty
        inl p2_nexts : ra u64 _ = am.empty
        inl pids : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions',next =>
                rm.add actions_indices i
                if pid = 0 then
                    rm.add p1_actions (player_state,game_state,pid,actions')
                    rm.add p1_nexts next
                else
                    rm.add p2_actions (player_state,game_state,pid,actions')
                    rm.add p2_nexts next
                rm.add pids pid
            | Terminal: x =>
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length pids then
                inl p1_cs,p1_update = p1 p1_actions
                inl p2_cs,p2_update = p2 p2_actions
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl p1_rs = am.slice (from:0 nearTo:length p1_cs) rs |> p1_update
                inl p2_rs = am.sliceFrom (length p1_cs) rs |> p2_update
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl neural_handler extractor (schema : schema _ _ _) p actions =
    !!!!Import("torch")
    inl policy_size = serialization.dense.array.size schema.policy
    inl value_size = serialization.dense.array.size schema.value
    inl action_size = serialization.sparse.int.size schema.action
    inl num_actions = length actions
    inl value_data : obj = $"torch.zeros(!num_actions,!value_size)"
    inl policy_data : obj = $"!value_data[:,:!policy_size]"
    inl action_mask : obj = $"torch.ones(!num_actions,!action_size,dtype=torch.bool)"
    actions |> am.iteri fun num_action (player_state,game_state,pid,actions') =>
        schema.value.pickle (extractor (player_state,game_state,pid)) $"!value_data[!num_action,:].numpy()"
        actions' |> am.iter (schema.action.pickle >> fun x => $"!action_mask[!num_action,!x] = False")
    inl x : obj = p (policy_data,value_data,action_mask)
    inl action_probs, sample_probs, sample_indices, update : obj * obj * obj * (obj * obj -> obj) = $"!x[0]", $"!x[1]", $"!x[2]", $"!x[3]"
    inl cs : a u64 _ = am.init num_actions fun b =>
        inl a : i64 = $"!sample_indices[!b]"
        log_prob_from {policy= $"!action_probs[!b,!a]"; sample= $"!sample_probs[!b,!a]"}, schema.action.unpickle a
    inl update (rewards : a u64 r2) : a u64 r2 =
        inl regret_prob : obj = $"torch.empty(!num_actions,1)"
        inl pids : obj = $"torch.empty(!num_actions,1)"
        actions |> am.iteri fun num_action (player_state,game_state,pid,actions') =>
            inl regret_prob' =
                inl prob = pl2_probs player_state pid
                inl env_prob = prob.chance +@ prob.op
                exp (-prob.self.sample + (env_prob.policy - env_prob.sample))
            $"!regret_prob[!num_action,0] = !regret_prob'"
            inl pid = if pid = 0 then 1f32 else -1
            $"!pids[!num_action,0] = !pid"
        inl rewards : obj = $"torch.from_numpy(!rewards).unsqueeze(-1) * !pids"
        inl rewards = update (rewards,regret_prob)
        $"(!rewards * !pids).squeeze(-1).numpy()"
    cs,update