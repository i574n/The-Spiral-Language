inl vs_self' forall t. game (value_pickle : _ -> a u64 f32 -> ()) (policy_size : i64) (value_size : i64) 
        (action_pickle : t -> i64) (action_unpickle : i64 -> t) (action_size : i64) 
        (batch_size, run) =
    let rec loop (l : a u64 _) =
        inl num_rewards, num_actions = 
            am.fold (fun (num_rewards,num_actions) => function
                | Action: player_state,game_state,pid,actions',next => num_rewards,num_actions+1
                | Terminal: x => num_rewards+1,num_actions
                ) (0,0) l
        inl rewards : a u64 _ = create num_rewards
        inl actions_indices : a u64 _ = create num_actions
        inl nexts : a u64 _ = create num_actions 
        inl value_data : obj = $"torch.zeros(!num_actions,!value_size)"
        inl policy_data : obj = $"!value_data[:,:!policy_size]"
        inl action_mask : obj = $"torch.ones(!num_actions,!action_size,dtype=torch.bool)"
        inl regret_prob : obj = $"torch.empty(!num_actions,1)"
        inl pids : obj = $"torch.empty(!num_actions,1)"
        l |> am.fold (fun (i,num_reward,num_action) => function
            | Action: player_state,game_state,pid,actions',next => 
                set actions_indices num_action i
                value_pickle (player_state,game_state,pid) $"!value_data[!num_action,:].numpy()"
                actions' |> am.iter (action_pickle >> fun x => $"!action_mask[!num_action,!x] = False")
                inl regret_prob' =
                    inl prob = pl2_probs player_state pid
                    inl env_prob = prob.chance +@ prob.op
                    exp (-prob.self.sample + (env_prob.policy - env_prob.sample))
                $"!regret_prob[!num_action,0] = !regret_prob'"
                set nexts num_action next
                inl pid = if pid = 0 then 1f32 else -1
                $"!pids[!num_action,0] = !pid"
                i+1,num_reward,num_action+1
            | Terminal: x => 
                set rewards num_reward (i,x)
                i+1,num_reward+1,num_action
            ) (0,0,0) |> ignore
        inl rewards_actions : a _ r2 =
            if 0 < num_actions then
                inl x : obj = run (policy_data,value_data,action_mask)
                inl action_probs, sample_probs, sample_indices, update : obj * obj * obj * (obj * obj -> obj) = $"!x[0]", $"!x[1]", $"!x[2]", $"!x[3]"
                inl cs = am.init num_actions fun b => 
                    inl a : i64 = $"!sample_indices[!b]"
                    log_prob_from {policy= $"!action_probs[!b,!a]"; sample= $"!sample_probs[!b,!a]"}, action_unpickle a
                inl rewards = am.map2 (<|) nexts cs |> loop
                inl rewards : obj = $"torch.from_numpy(!rewards).unsqueeze(-1) * !pids"
                inl rewards = update (rewards,regret_prob)
                $"(!rewards * !pids).squeeze(-1).numpy()"
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl vs_self game (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl actions : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions',next => 
                rm.add actions_indices i
                rm.add actions (player_state,game_state,pid,actions')
                rm.add nexts next
            | Terminal: x => 
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length actions then
                inl cs,update = p actions
                am.generic.map2 (<|) nexts cs |> loop |> update
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)

inl vs_one game (batch_size, p1, p2) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl p1_actions : ra u64 _ = am.empty
        inl p2_actions : ra u64 _ = am.empty
        inl p1_nexts : ra u64 _ = am.empty
        inl p2_nexts : ra u64 _ = am.empty
        inl pids : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions',next =>
                rm.add actions_indices i
                if pid = 0 then
                    rm.add p1_actions (player_state,game_state,pid,actions')
                    rm.add p1_nexts next
                else
                    rm.add p2_actions (player_state,game_state,pid,actions')
                    rm.add p2_nexts next
                rm.add pids pid
            | Terminal: x =>
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length pids then
                inl p1_cs,p1_update = p1 p1_actions
                inl p2_cs,p2_update = p2 p2_actions
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl p1_rs = am.slice (from:0 nearTo:length p1_cs) rs |> p1_update
                inl p2_rs = am.sliceFrom (length p1_cs) rs |> p2_update
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)