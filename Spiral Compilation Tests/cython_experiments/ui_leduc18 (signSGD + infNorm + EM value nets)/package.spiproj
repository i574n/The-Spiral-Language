// Since the past attempts failed, I am going to try something else here. Inspired by the 
// signSGN papers, for the optimizer I am going to make an update that interpolates 
// signSGD + infinity norm gradient normalization. For the value network, since I absolutely 
// need the values to act as weighted moving averages, I am going to semi-tabular CFR in the 
// value head.

// Instead of one hot vectors like in full tabular, the semi tabular will have probabilistic 
// vectors. I'll take a log softmax over the body, exp it and use that as the prob vector input 
// to the head which be optimized using the tabular algorithm.

// For getting the gradients for the value body, I'll use the absolute value differential
// between the given and the predicted error, center it with regards to the probabilistic mean
// and use that as the backwards input for the log softmax.
// (Edit: As it turns out, the regular softmax works a lot better. I was worrying that saturation
// would be an issue, but that property actually makes convergence possible.)

// Alternatively updating the value head and body will make this an EM procedure similar to k-means.
// The main reason why I am going for this besides being able to weight the values is because
// right now I can't tell at all whether the value update works. If I had a tabular agent, this
// kind of debugging would be a lot easier.

// The issue with the value net is that it needs to learn the reward magnitudes in the weights,
// so I cannot use something like signSGD (which is Adam with full batch learning) to stabilize it.
// This is a source of many of my headaches. NNs are good for probability distributions, but bad
// for learning large ranges of values.

// With this method I'll be able to optimize the value head and get something useful even without
// necessarily optimizing the body if I want to pick that route. (Edit: Much like for the actor, 
// optimizing the critic body using the targets generated by the tabular algorithm works fine.)

// The actor on the other hand will be will be possible to deal with as I will be able to use policy
// gradients with the aforementioned optimizer together.

// ---

// Update (6/9/2021): My worked and I am capable of training a NN agent competitive with the tabular 
// one with roughly the same amount of compute. The NN algo itself is implemented in `control.py`.

// Some details:
// * Even though the optimizer supports interpolation, I've ditched infinity norm in favor of pure signSGD.
//   I might add a momentum component to signSGD in the future, but otherwise I won't bother with adaptive 
//   optimizers like Adam. They all converge to signSGD when doing full batch learning anyway.
//   In future work, I think I'll just tune the batch size until signSGD is happy.
// * Averaging in the weight space is sufficient for dampening self play oscilations and getting good policies.
//   Thanks to it I did not have to gradually lower the learning rate or anneal the exploration epsilon.
//   I did observe a benefit when I lowered the actor LR by 2x, so it should be possible to get better 
//   performance by doing LR annealing even with averaging.

// I am quite pleased with how things went here. I'll archive this project and move on to making 
// the Holdem game next. The algorithm here is novel and deserves a blog post at some point, though I 
// won't bother writing a paper as I am not into theorems.

// I am looking forward to trying it out on games that could give me real world advantages.

// Update (6/18/2021): Did some updating and cleaning up. As it turns out, I was looking the 
// regular players instead of the averaged ones and drawing the right conclussions from the 
// wrong data. I am lucky that what I wrote in the relevant bullet about weight averaging point 
// is all true.

// The commit at this date has data on what my runs look like. I've been training agents over and 
// over for the whole day. The only activations worth using with LN are Relu and Leaky Relu. 
// I tried a few others, but probably because of poor weight init they saturate too easily and 
// work very poorly. Relu composes with LN particularly well, and it will be my main choice. Relu, 
// Leaky Relu (and `abs` which I haven't tried) when paired with LN and signSGD have the property 
// of making gradient steps invariant to the weight scale. 

// The algorithm in `control.py` preserves that property when the goal is to learn value functions.

// Update (6/22/2021): As it turns out updating the critic, actor and the critic head all at once
// improves computation time by 4x without degrading the final performance. I am impressed.
// Compared to the `Actor-Critic Policy Optimization in Partially Observable Multiagent Environments`
// paper, my method here gives better final performance while being over a 100 times faster to train.
// In that paper they update the critic 128 times before updating the actor once.

// I haven't bothered testing this until now, but since I am preping for Holdem, I realized that
// I really should since if it worked, it could reduce the time it takes to train an agent from a
// month to a week. One hyperparam I still haven't bothered tuning at all is the head decay.

// A batch of 1024 is enough to cover all of the Leduc's states so I haven't bothered, but for Holdem
// and bigger games 2 ** -2 might be too strong of a decay.

packages: |core2-
modules:
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    leduc
    train
    agent/
        uniform
        neural
        tabular_old
        tabular
    create_args