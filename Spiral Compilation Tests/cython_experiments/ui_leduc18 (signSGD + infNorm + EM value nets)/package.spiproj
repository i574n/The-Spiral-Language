// Since the past attempts failed, I am going to try something else here.
// Inspired by the signSGN papers, for the optimizer I am going to make an update 
// that interpolates signSGD + infinity norm gradient normalization.
// For the value network, since I absolutely need the values to act as weighted
// moving averages, I am going to semi-tabular CFR in the value head.

// Instead of one hot vectors like in full tabular, the semi tabular will have 
// probabilistic vectors. I'll take a log softmax over the body, exp it and use
// that as the input to the head which be optimized using the tabular algorithm.

// For getting the gradients for the value body, I'll use the absolute value differential
// between the given and the predicted error, center it with regards to the probabilistic mean
// and use that as the backwards input for the log softmax.

// Iterating updating the value head and body will make this an EM procedure similar to k-means.
// The main reason why I am going for this besides being able to weight the values is because
// right now I can't tell at all whether the value update works. If I had a tabular agent, this
// kind of debugging would be a lot easier.

// The issue with the value net is that it needs to learn the reward magnitudes in the weights,
// so I cannot use something like signSGD (which is Adam with full batch learning) to stabilize it.
// This is a source of many of my headaches. NNs are good for probability distributions, but bad
// for learning large ranges of values.

// With this method I'll be able to optimize the value head and get something useful even without
// necessarily optimizing the body.

// The actor on the other hand will be a lot easier to deal with as I will be able to use plain
// backprop with the aforementioned optimizer.

// ---

// Update (6/9/2021): My worked and I am capable of training a NN agent competitive with the tabular one with
// roughly the same amount of compute. The details are in the commit at this date.
// The NN algo itself is implemented in `control.py`.

// Some details:
// * Even though the optimizer supports interpolation, I've ditched infinity norm in favor of pure signSGD.
//   I might add a momentum component to signSGD in the future, but otherwise I won't bother with adaptive optimizers like Adam.
//   They all converge to signSGD when doing full batch learning anyway.
//   In future work, I think I'll just tune the batch size until signSGD is happy.
// * Averaging in the weight space is sufficient for dampening self play oscilations and getting good policies.
//   Thanks to it I did not have to gradually lower the learning rate or anneal the exploration epsilon.
//   I did observe a benefit when I lowered the actor LR by 2x, so it should be possible to get better performance 
//   by doing LR annealing even with averaging.

// I am quite pleased with how things went here. I'll archive this project and move on to making the Holdem game next.
// The algorithm here is novel and deserves a blog post at some point, though I won't bother writing a paper 
// as I am not into theorems.

// I am looking forward to trying it out on games that could give me real world advantages.

packages: |core2-
modules:
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    leduc
    train
    agent/
        uniform
        neural
        tabular_old
        tabular
    create_args