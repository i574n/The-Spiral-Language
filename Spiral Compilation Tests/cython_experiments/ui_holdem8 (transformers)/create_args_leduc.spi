inl main () =
    open leduc
    inl vs_self = train.vs_self leduc.game
    inl vs_one = train.vs_one leduc.game
    inl neural = train.neural agent.neural_leduc.extractor agent.neural_leduc.schema()
    inl uniform_player : ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
        agent.uniform.policy
    inl tabular =
        inl create_policy : agent.tabular.agent_dict (list (choice2 card action)) card * bool * bool * f32 -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            agent.tabular.policy agent.neural_leduc.extractor'
        inl create_agent () : agent.tabular.agent_dict (list (choice2 card action)) card = dictm.empty
        inl optimize : agent.tabular.agent_dict (list (choice2 card action)) card -> () = agent.tabular.optimize
        inl average : agent.tabular.agent_dict (list (choice2 card action)) card * agent.tabular.agent_dict (list (choice2 card action)) card -> () = agent.tabular.average
        inl head_multiply_ : agent.tabular.agent_dict (list (choice2 card action)) card * _ -> _ = agent.tabular.head_multiply_
        namedtuple "Tabular" {create_policy create_agent head_multiply_ optimize average}

    record {vs_self vs_one neural uniform_player tabular}