// Before I move to training on Holdem, I want to investigate why the nets blow up after training a while.
// This happened to me even on `simple_test.py`. And it happens on Leduc if I let it train for too long.
// I am playing with a script and I can see that passing in too large inputs into the softmax just causes it
// to saturate and block the gradients. And this is fine. Just how is it possible that I'd get nans when
// optimizing with signSGD? Given the moves possible in the weight space, it is not like they should grow
// unbounded.

// Update: The reason it is blowing up is because of head decay. If one of the state probabilities consistently
// becomes zero that will cause the head_weight to become zero and that quantity is used as the denominator.
// This then causes nans to show up.

// This explains the instability I've observed in `simple_test.py`, but is probably not the reason why training 
// blew that one time with the 2 ** -8 learning rate.

// I am going to update torch to 1.9.0 and use `nan_to_num` to make sure this problem can never happen again. I'll 
// also update the previous project.

packages: |core2-
modules:
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    leduc
    train
    agent/
        uniform
        neural
        tabular_old
        tabular
    create_args