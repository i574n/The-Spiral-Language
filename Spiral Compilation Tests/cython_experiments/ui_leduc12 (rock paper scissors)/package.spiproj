// I want to test my implicit averaging idea, but training against the current policy works too well on Leduc.
// I know from experience that the current policy oscillates wildly in rock paper scissors, so let me try that instead.
// The stuff here should not be a reference.

// Update: Noise keying is a bust, but optimizing a bucket of policies is a win.
// This tells me that I should be able to just use dropout to get the benefits of policy averaging.
// That is what I am going to do with NNs.
packages: |core2-
modules:
    utils-
    log_probm-
    sampling
    game-
    wval-
    nodes/
        utils-
        cps
        main-
    leduc
    rps 
    agent/
        uniform
        cfr_enum
        cfr_enum_experimental
        cfr_enum_experimental2
        cfr_sample_learned_infoset
        cfr_sample_learned_history
        human
    rps_test