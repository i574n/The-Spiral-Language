inl main () =
    open leduc
    inl vs_self = train.vs_self leduc.game
    inl vs_one = train.vs_one leduc.game
    inl neural =
        open agent.neural_ff_leduc
        inl schema = schema()
        inl handler = train.neural_handler (present_ex,future_ex) schema
        inl size =
            inl present = serialization.dense.array.size schema.present
            inl future = serialization.dense.array.size schema.future
            inl action = serialization.sparse.int.size schema.action
            namedtuple "Size" {action present future}
        namedtuple "Neural" {handler size}
    inl uniform_player : ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * _ =
        agent.uniform.policy
    inl tabular =
        inl create_policy : agent.tabular.agent_dict (list (observation card action)) card * bool * bool * f32 -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * _ =
            agent.tabular.policy agent.neural_ff_leduc.tabular_ex
        inl create_agent () : agent.tabular.agent_dict (list (observation card action)) card = dictm.empty
        inl optimize : agent.tabular.agent_dict (list (observation card action)) card -> () = agent.tabular.optimize
        inl average : agent.tabular.agent_dict (list (observation card action)) card * agent.tabular.agent_dict (list (observation card action)) card -> () = agent.tabular.average
        inl head_multiply_ : agent.tabular.agent_dict (list (observation card action)) card * _ -> _ = agent.tabular.head_multiply_
        namedtuple "Tabular" {create_policy create_agent head_multiply_ optimize average}

    record {vs_self vs_one neural uniform_player tabular}